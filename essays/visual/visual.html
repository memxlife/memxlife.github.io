<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <title>Geometry Before Abstraction</title>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <!-- Markdown renderer -->
  <script defer src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Arial, sans-serif;
      max-width: 800px;
      margin: 60px auto;
      line-height: 1.6;
      color: #222;
    }
    h1 { font-size: 2.2em; }
    h2 { margin-top: 40px; }
    a { color: #0366d6; text-decoration: none; }
    a:hover { text-decoration: underline; }
    img { max-width: 100%; }
  </style>
</head>

<body>
  <div id="content">Loading…</div>

  <!-- Embedded Markdown: paste visual.md content here -->
  <script id="md-source" type="text/markdown">
---
title: Geometry Before Abstraction
---

![Banner](banner.jpg)

# Geometry Before Abstraction  
## Multi-Scale Visual–Spatial Co-Learning

We argue that embodied intelligence is fundamentally a geometric epistemology. An agent does not first learn symbols and then attach them to the world; it first acquires a representation whose internal topology is aligned with the topology of physical interaction. From this alignment, higher-level abstractions become compressions of stable geometric relations rather than free-floating codes. This position yields a disciplined consequence: representation learning for embodied agents should preserve spatial topology across scales, treat temporal change primarily as transport of structure, and allow abstraction only through topology-preserving coarse-graining. We formalize a minimal set of physical assumptions under which geometry-aligned learning is well-posed and illustrate constructively how approximate metric cognition, such as estimating “ten steps,” can emerge from geometry-preserving visual fields via calibration to reference units, without requiring explicit symbolic three-dimensional reconstruction. Our claim is not that geometry is sufficient for all cognition, but that it is a necessary substrate. Abstraction that precedes geometric grounding tends to be brittle, data-hungry, and unstable for control.

## Position: Embodied Learning as Geometric Epistemology

The dominant temptation in modern machine learning is to treat representation as an unconstrained coding problem. Any latent that supports prediction is considered acceptable. In embodied settings, this temptation is costly, because the world is not an arbitrary data distribution. It is a physical system governed by invariants: continuity in space and time, locality of interaction, monotonic projection cues under approach, and structured coarse-to-fine organization in both perception and action.

These invariants are not statistical accidents. They are the structural preconditions that make control possible. A representation that ignores them can still interpolate within a dataset, but it cannot reliably generalize across physically plausible perturbations. It cannot become a stable model of interaction.

Our position is therefore both philosophical and technical. Philosophically, an embodied agent “knows” space when its internal representation admits the same adjacency and ordering relations that the world enforces through contact and motion. Technically, this is best understood not as explicit coordinate reconstruction but as topology alignment. The geometry of representation should be a structured deformation of the geometry of the world. When this alignment holds, learning becomes alignment with existing structure rather than rediscovery of structure, and abstraction becomes compression rather than invention.

This is not an argument against abstraction. It is an argument about order. Geometry must precede symmetry, and symmetry must precede abstraction. Abstraction introduced before geometric grounding tends to entangle spatial relations into global codes, increasing sample complexity and destabilizing control. Abstraction introduced after geometry has stabilized can be powerful precisely because it compresses a representation that is already physically meaningful.

## Why Visual–Spatial Co-Learning Is the Correct Framing

It is common to divide perception into separate subproblems: visual features for appearance and spatial features for location and motion. In embodied systems, this division is misleading. Appearance is not separable from space because motion transports appearance, identity is defined by persistence over time, and objecthood emerges from coherent regions whose boundaries are visually determined but whose relations are spatial. Conversely, spatial reasoning cannot proceed without appearance, because regions and correspondences must be anchored in discriminable structure.

Visual learning and spatial learning therefore form a coupled constraint system acting on a single representation. The learner must discover channels that are appearance-sensitive and at the same time stable under transport. It must discover spatial structure through appearance-defined regions embedded in a geometry-preserving lattice. This coupling is multi-scale because the world is multi-scale. Fine control requires high-frequency geometric precision, while robust identity and layout require coarser invariances. The central principle is that invariance is not permission to destroy geometry. It must arise through topology-preserving coarse-graining.

## Minimal Physical Assumptions

Assume there exists a low-dimensional task-relevant physical state $s_t \in \mathcal{M}$, and observations satisfy

$$
I_t = \Pi(s_t) + \eta_t.
$$

Assume further that physical dynamics are piecewise smooth, so that

$$
s_{t+1} = g(s_t, a_t) + \epsilon_t.
$$

## Formal Core: Geometry-Preserving Fields

Representation is modeled as a hierarchy of retinotopic feature fields

$$
F_t^{(\ell)} \in \mathbb{R}^{H_\ell \times W_\ell \times C_\ell}.
$$

Across scales, abstraction is enforced through local aggregation:

$$
F_t^{(\ell+1)} \approx A^{(\ell)}(F_t^{(\ell)}).
$$

## Constructive Derivation: From Fields to Metric Cognition

Under a pinhole camera model with focal length $f$,

$$
r \approx f \frac{R}{Z},
$$

which implies

$$
Z \approx \frac{fR}{r}.
$$

Angular separation satisfies

$$
\theta \approx \frac{\sqrt{(u_2 - u_1)^2 + (v_2 - v_1)^2}}{f}.
$$

An approximate center distance follows:

$$
\widehat{D}^2 \approx 
\widehat{Z}_1^2 + \widehat{Z}_2^2 
- 2 \widehat{Z}_1 \widehat{Z}_2 \cos\theta.
$$

Metric cognition then emerges through calibration:

$$
\widetilde{D} = \mathcal{C}(\|\Delta\|, r_1, r_2; \theta_C).
$$

## Falsifiability

A serious position must make predictions. The geometry-first view predicts that strong embodied systems will exhibit scales at which low-complexity readouts recover spatial variables, that rollout error will degrade gradually with horizon under physically plausible perturbations, and that coarse-scale invariances will remain traceable to local aggregation of fine-scale fields.

## Outlook

Geometry is not the ceiling of intelligence. It is the substrate. Memory, object-centric latents, predictive world models, and planning become stable only when operating on geometry-preserving representations. Without such stabilization, long-horizon reasoning amplifies error and decision-making becomes brittle.
  </script>

  <script>
    window.addEventListener('DOMContentLoaded', async () => {
      const raw = document.getElementById('md-source').textContent;

      // strip YAML front matter if present
      const md = raw.replace(/^---[\s\S]*?---\s*/m, '');

      document.getElementById('content').innerHTML = marked.parse(md);

      if (window.MathJax && MathJax.typesetPromise) {
        await MathJax.typesetPromise();
      }
    });
  </script>
</body>
</html>
